{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Union, List, Tuple, Any\n",
    "# from tabulate import tabulate\n",
    "\n",
    "class AuditLog:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = {column: None for column in self.columns}\n",
    "        self.log['STARTTIME'] = datetime.now() + timedelta(hours=7)\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.log)\n",
    "    \n",
    "class AuditLog_SPC(AuditLog):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId, TriggerType, TableName, functionName, ):\n",
    "        super().initialDetail({'PIPELINENAME': pipelineName, 'PIPELINERUNID': pipelineId, 'TRIGGERTYPE': TriggerType, 'TABLE_NAME': TableName, 'FUNCTION_NAME': functionName})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AuditLog_SPC('SPC_UAT', 'factTest', 'auditTable', 'SilverLH', 'AuditLH')\n",
    "ad.initialDetail('testPL', '123', 'manual', 'factTest', 'testFunction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STATUS_ACTIVITY': 'Not start', 'TRIGGERTYPE': None, 'ERRORCODE': None, 'STARTTIME': datetime.datetime(2025, 1, 18, 4, 50, 58, 114075), 'ENDTIME': None, 'PIPELINERUNID': None, 'TABLE_NAME': None, 'ERRORMESSAGE': None, 'PIPELINENAME': None, 'AUDITKEY': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'FUNCTION_NAME': None}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenKeysDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = FrozenKeysDict(**{column: None for column in ['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log['PIPELINENAME'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIPELINENAME': 'xxx', 'PIPELINERUNID': None, 'TRIGGERTYPE': None, 'TABLE_NAME': None, 'FUNCTION_NAME': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'ERRORCODE': None, 'ERRORMESSAGE': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Cannot add new key: new'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myyy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mFrozenKeysDict.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data:\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot add new key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cannot add new key: new'"
     ]
    }
   ],
   "source": [
    "log['new']='yyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logger:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "\n",
    "class AuditLog:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str, ...]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = logger(**{column: None for column in self.columns})\n",
    "        self.log['STARTTIME'] = datetime.now() + timedelta(hours=7)\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "\n",
    "    def setKey(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        self.setKey(initConfig)\n",
    "\n",
    "    def getKey(self):\n",
    "        return self.columns\n",
    "    \n",
    "    def getLog(self):\n",
    "        return self.log\n",
    "        \n",
    "    def __str__(self):\n",
    "        out = ''\n",
    "        for key in self.columns:\n",
    "            out += f'{key}: {self.log[key]}\\n'\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.log)\n",
    "\n",
    "\n",
    "class AuditLog_SPC(AuditLog):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId, TriggerType, TableName, functionName, ):\n",
    "        super().initialDetail({\n",
    "            'PIPELINENAME': pipelineName, \n",
    "            'PIPELINERUNID': pipelineId, \n",
    "            'TRIGGERTYPE': TriggerType, \n",
    "            'TABLE_NAME': TableName, \n",
    "            'FUNCTION_NAME': functionName\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STATUS_ACTIVITY': 'Not start', 'TRIGGERTYPE': 'manual', 'ERRORCODE': None, 'STARTTIME': datetime.datetime(2025, 1, 18, 20, 19, 37, 815088), 'ENDTIME': None, 'PIPELINERUNID': '123', 'TABLE_NAME': 'factTest', 'ERRORMESSAGE': None, 'PIPELINENAME': 'testPL', 'AUDITKEY': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'FUNCTION_NAME': 'testFunction'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 20:19:37.815088\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, when, concat, lit, format_string,sum, upper, substring, expr, current_date, current_timestamp,to_timestamp,concat_ws, isnull, date_format, asc, trim, trunc, date_sub, year,coalesce, count, countDistinct, min, max\n",
    "from pyspark.sql.types import IntegerType, DecimalType, StringType, LongType, TimestampType, StructType, StructField, DoubleType, FloatType\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Union, List, Tuple, Any\n",
    "# from tabulate import tabulate\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"utils\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "class logger:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "    \n",
    "# class Audit:\n",
    "#     def __init__(self):\n",
    "#         raise NotImplementedError('This is an abstract class')\n",
    "    \n",
    "#     def setKeys(self, initConfig: dict[str, Any]):\n",
    "#         assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "#         for column in initConfig:\n",
    "#             self.log[column] = initConfig[column]\n",
    "\n",
    "        \n",
    "\n",
    "class AuditLog_Fusion:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str, ...]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = logger(**{column: None for column in self.columns})\n",
    "        self.log['STARTTIME'] = datetime.now()\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "\n",
    "    def setKeys(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "\n",
    "    def setKey(self, key: str, value: Any):\n",
    "        assert key in self.columns, f'key must be in {self.columns}'\n",
    "        self.log[key] = value\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        self.setKeys(initConfig)\n",
    "\n",
    "    def getKey(self):\n",
    "        return self.columns\n",
    "    \n",
    "    def getLog(self):\n",
    "        return self.log\n",
    "        \n",
    "    def __str__(self):\n",
    "        out = ''\n",
    "        for key in self.columns:\n",
    "            out += f'{key}: {self.log[key]}\\n'\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.log)\n",
    "    \n",
    "    def endSuccess(self):\n",
    "        self.log['STATUS_ACTIVITY'] = 'Fail'\n",
    "        self._endAuditLog()\n",
    "        print(self)\n",
    "        \n",
    "    def endFail(self, errorCode: str, errorMessage: str):\n",
    "        self.log['STATUS_ACTIVITY'] = 'Fail'\n",
    "        self.log['ERRORCODE'] = errorCode\n",
    "        self.log['ERRORMESSAGE'] = errorMessage\n",
    "        self._endAuditLog()\n",
    "        print(self)\n",
    "\n",
    "    def _endAuditLog(self):\n",
    "        # write to audit table\n",
    "        self.log['ENDTIME'] = datetime.now()\n",
    "        df = spark.createDataFrame([self.log])\n",
    "        df.write.mode('append').save(self.PATH_TO_AUDIT_TABLE)\n",
    "\n",
    "    def getAuditLogTable(self):\n",
    "        return spark.read.load(self.PATH_TO_AUDIT_TABLE)\n",
    "    \n",
    "    def countBefore(self, df):\n",
    "        self.log['COUNTROWSBEFORE'] = df.count()\n",
    "\n",
    "    def countAfter(self, df):\n",
    "        self.log['COUNTROWSAFTER'] = df.count()\n",
    "\n",
    "    def getAllPath(self):\n",
    "        return {'PATH_TO_AUDIT_TABLE':self.PATH_TO_AUDIT_TABLE, 'PATH_TO_CHECKED_TABLE':self.PATH_TO_CHECKED_TABLE}\n",
    "\n",
    "\n",
    "class AuditLog_SPC(AuditLog_Fusion):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId: str, TriggerType: str, TableName: str, functionName: str):\n",
    "        super().initialDetail({\n",
    "            'PIPELINENAME': pipelineName, \n",
    "            'PIPELINERUNID': pipelineId, \n",
    "            'TRIGGERTYPE': TriggerType, \n",
    "            'TABLE_NAME': TableName, \n",
    "            'FUNCTION_NAME': functionName\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 13:40:43.660532\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad = AuditLog_SPC(WS_ID = 'SPC_UAT', TABLE_NAME_to_check = 'factTest', AUDIT_TABLE_NAME='auditTable', LH_ID_to_check='SilverLH', LH_ID_audit='AuditLH')\n",
    "ad.initialDetail(pipelineName = 'testPL', pipelineId = '123', TriggerType = 'manual', TableName = 'factTest', functionName = 'testFunction')\n",
    "print(ad)\n",
    "\n",
    "raise FileExistsError('Create you audit table first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: bigint]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoDf = spark.createDataFrame([{'a':1, 'b':2}, {'a':3, 'b':4}])\n",
    "demoDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 13:39:16.665795\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notebookutils.fs.exists(ad.PATH_TO_AUDIT_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "อีเมลถูกส่งสำเร็จ!\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# ข้อมูลการตั้งค่าของผู้ส่ง\n",
    "gmail_user = 'phaphontee.yam@student.mahidol.edu'         # ใส่ที่อยู่อีเมลของคุณ\n",
    "app_password = 'rziq gbow xoer qlnv'          # ใส่ App Password ที่สร้างไว้\n",
    "\n",
    "# ข้อมูลของผู้รับและเนื้อหาอีเมล\n",
    "to = 'phaphontee@fusionsol.com'          # ใส่อีเมลผู้รับ\n",
    "subject = 'ทดสอบการส่งอีเมลด้วย Python'   # ใส่หัวเรื่อง\n",
    "body = 'สวัสดีครับ นี่เป็นอีเมลทดสอบที่ส่งด้วย Python!'  # ใส่เนื้อหาของอีเมล\n",
    "\n",
    "# การสร้างอีเมล\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = gmail_user\n",
    "msg['To'] = to\n",
    "msg['Subject'] = subject\n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "try:\n",
    "    # การเชื่อมต่อกับ SMTP ของ Gmail\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    server.starttls()  # เปิดการเข้ารหัส TLS\n",
    "    server.login(gmail_user, app_password)  # เข้าสู่ระบบด้วยอีเมลและรหัสผ่านแอป\n",
    "    text = msg.as_string()\n",
    "\n",
    "    # ส่งอีเมล\n",
    "    server.sendmail(gmail_user, to, text)\n",
    "    print(\"อีเมลถูกส่งสำเร็จ!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"เกิดข้อผิดพลาด: {e}\")\n",
    "\n",
    "finally:\n",
    "    # ปิดการเชื่อมต่อ\n",
    "    server.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, when, concat, lit, format_string,sum, upper, substring, expr, current_date, current_timestamp,to_timestamp,concat_ws, isnull, date_format, asc, trim, trunc, date_sub, year,coalesce, count, countDistinct, min, max\n",
    "from pyspark.sql.types import IntegerType, DecimalType, StringType, LongType, TimestampType, StructType, StructField, DoubleType, FloatType\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Union, List, Tuple, Any\n",
    "# from tabulate import tabulate\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"test\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl.factCPU import CashPickUp_SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>St</th>\n",
       "      <th>Reference</th>\n",
       "      <th>BP</th>\n",
       "      <th>G_L</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Assignment</th>\n",
       "      <th>DocumentNo</th>\n",
       "      <th>Ty</th>\n",
       "      <th>Doc Date</th>\n",
       "      <th>PK</th>\n",
       "      <th>...</th>\n",
       "      <th>Clrng doc</th>\n",
       "      <th>Materia</th>\n",
       "      <th>Text</th>\n",
       "      <th>AssignmentDate</th>\n",
       "      <th>DocDate</th>\n",
       "      <th>POSNumber</th>\n",
       "      <th>AmountCURR</th>\n",
       "      <th>FileName</th>\n",
       "      <th>ETL_Date</th>\n",
       "      <th>YYYYMM_DocDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [St, Reference, BP, G_L, Cost, Assignment, DocumentNo, Ty, Doc Date, PK, Amount in doc curr, Quantity, Clrng doc, Materia, Text, AssignmentDate, DocDate, POSNumber, AmountCURR, FileName, ETL_Date, YYYYMM_DocDate]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"1106070_25_03_13.XLSx\"\n",
    "df = pd.read_excel(filename,skiprows=4).fillna(\"\").astype(\"str\").sum(axis=1).str.split(\"\\t\",expand=True)\n",
    "df_col = df.iloc[0].str.replace('.','').str.replace('G/L','G_L')\n",
    "df = df.iloc[2:]\n",
    "df.columns = df_col.str.strip().apply(lambda x: '-' if len(x) == 0 else x)\n",
    "df = df.iloc[:-2]\n",
    "df = df.drop(columns='-')\n",
    "df = df[df['Text'].str.contains('CPICK')]\n",
    "df['AssignmentDate'] = df['Assignment'].str[-8:]\n",
    "df['DocDate'] = pd.to_datetime(df['Doc Date'].str.replace(\".\",\"\"),format=\"%m%d%Y\").dt.strftime('%Y%m%d')\n",
    "df['POSNumber'] = df['Assignment'].str[:5]\n",
    "df['Amount in doc curr'] = df['Amount in doc curr'].str.strip()\n",
    "df['DocumentNo'] = df['DocumentNo'].str.strip()\n",
    "df['AmountCURR'] = df['Amount in doc curr'].str.extract(r'([^\\.]+)\\..*',expand=False).str.replace(r'[\\.,]',\"\",regex=True)\n",
    "df['FileName'] = filename\n",
    "df['ETL_Date'] = datetime.now()\n",
    "df['YYYYMM_DocDate'] = pd.to_datetime(df['Doc Date'],format='%d.%m.%Y').dt.strftime('%Y%m')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>St</th>\n",
       "      <th>Reference</th>\n",
       "      <th>BP</th>\n",
       "      <th>G_L</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Assignment</th>\n",
       "      <th>DocumentNo</th>\n",
       "      <th>Ty</th>\n",
       "      <th>Doc Date</th>\n",
       "      <th>PK</th>\n",
       "      <th>...</th>\n",
       "      <th>Clrng doc</th>\n",
       "      <th>Materia</th>\n",
       "      <th>Text</th>\n",
       "      <th>AssignmentDate</th>\n",
       "      <th>DocDate</th>\n",
       "      <th>POSNumber</th>\n",
       "      <th>AmountCURR</th>\n",
       "      <th>FileName</th>\n",
       "      <th>ETL_Date</th>\n",
       "      <th>YYYYMM_DocDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0184</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>1001920250301</td>\n",
       "      <td>5106004555</td>\n",
       "      <td>GC</td>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01.03.2025ปป.ยอดเงินดรอป10019</td>\n",
       "      <td>20250301</td>\n",
       "      <td>20250103</td>\n",
       "      <td>10019</td>\n",
       "      <td>-991</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0035</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>1003220250301</td>\n",
       "      <td>5106004559</td>\n",
       "      <td>GC</td>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01.03.2025ปป.ยอดเงินดรอป10032</td>\n",
       "      <td>20250301</td>\n",
       "      <td>20250103</td>\n",
       "      <td>10032</td>\n",
       "      <td>-50</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0014</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>1004020250301</td>\n",
       "      <td>5106004561</td>\n",
       "      <td>GC</td>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01.03.2025ปป.ยอดเงินดรอป10040</td>\n",
       "      <td>20250301</td>\n",
       "      <td>20250103</td>\n",
       "      <td>10040</td>\n",
       "      <td>224167</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0059</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>1008520250301</td>\n",
       "      <td>5106004565</td>\n",
       "      <td>GC</td>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01.03.2025ปป.ยอดเงินดรอป10085</td>\n",
       "      <td>20250301</td>\n",
       "      <td>20250103</td>\n",
       "      <td>10085</td>\n",
       "      <td>136861</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0257</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>1010420250301</td>\n",
       "      <td>5106004569</td>\n",
       "      <td>GC</td>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>01.03.2025ปป.ยอดเงินดรอป10104</td>\n",
       "      <td>20250301</td>\n",
       "      <td>20250103</td>\n",
       "      <td>10104</td>\n",
       "      <td>-2584</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td></td>\n",
       "      <td>22220</td>\n",
       "      <td>0031</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>2222020250312</td>\n",
       "      <td>5302021396</td>\n",
       "      <td>G3</td>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDRP1 เงินสด Drop ลงเซฟ</td>\n",
       "      <td>20250312</td>\n",
       "      <td>20251203</td>\n",
       "      <td>22220</td>\n",
       "      <td>65643</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td></td>\n",
       "      <td>22413</td>\n",
       "      <td>0168</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>2241320250312</td>\n",
       "      <td>5302021398</td>\n",
       "      <td>G3</td>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDRP1 เงินสด Drop ลงเซฟ</td>\n",
       "      <td>20250312</td>\n",
       "      <td>20251203</td>\n",
       "      <td>22413</td>\n",
       "      <td>34419</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td></td>\n",
       "      <td>22430</td>\n",
       "      <td>0207</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>2243020250312</td>\n",
       "      <td>5302021399</td>\n",
       "      <td>G3</td>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDRP1 เงินสด Drop ลงเซฟ</td>\n",
       "      <td>20250312</td>\n",
       "      <td>20251203</td>\n",
       "      <td>22430</td>\n",
       "      <td>71098</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td></td>\n",
       "      <td>22442</td>\n",
       "      <td>0231</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>2244220250312</td>\n",
       "      <td>5302021400</td>\n",
       "      <td>G3</td>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDRP1 เงินสด Drop ลงเซฟ</td>\n",
       "      <td>20250312</td>\n",
       "      <td>20251203</td>\n",
       "      <td>22442</td>\n",
       "      <td>160493</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td></td>\n",
       "      <td>22466</td>\n",
       "      <td>0267</td>\n",
       "      <td>1000020</td>\n",
       "      <td></td>\n",
       "      <td>2246620250312</td>\n",
       "      <td>5302021401</td>\n",
       "      <td>G3</td>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>CDRP1 เงินสด Drop ลงเซฟ</td>\n",
       "      <td>20250312</td>\n",
       "      <td>20251203</td>\n",
       "      <td>22466</td>\n",
       "      <td>151284</td>\n",
       "      <td>1000020_25_03_13.XLSx</td>\n",
       "      <td>2025-03-31 20:02:23.208218</td>\n",
       "      <td>202503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3899 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0    St Reference    BP      G_L Cost     Assignment  DocumentNo  Ty  \\\n",
       "2                  0184  1000020       1001920250301  5106004555  GC   \n",
       "3                  0035  1000020       1003220250301  5106004559  GC   \n",
       "4                  0014  1000020       1004020250301  5106004561  GC   \n",
       "5                  0059  1000020       1008520250301  5106004565  GC   \n",
       "6                  0257  1000020       1010420250301  5106004569  GC   \n",
       "...  ..       ...   ...      ...  ...            ...         ...  ..   \n",
       "3896        22220  0031  1000020       2222020250312  5302021396  G3   \n",
       "3897        22413  0168  1000020       2241320250312  5302021398  G3   \n",
       "3898        22430  0207  1000020       2243020250312  5302021399  G3   \n",
       "3899        22442  0231  1000020       2244220250312  5302021400  G3   \n",
       "3900        22466  0267  1000020       2246620250312  5302021401  G3   \n",
       "\n",
       "0       Doc Date  PK  ... Clrng doc Materia                           Text  \\\n",
       "2     01.03.2025  50  ...                    01.03.2025ปป.ยอดเงินดรอป10019   \n",
       "3     01.03.2025  50  ...                    01.03.2025ปป.ยอดเงินดรอป10032   \n",
       "4     01.03.2025  40  ...                    01.03.2025ปป.ยอดเงินดรอป10040   \n",
       "5     01.03.2025  40  ...                    01.03.2025ปป.ยอดเงินดรอป10085   \n",
       "6     01.03.2025  50  ...                    01.03.2025ปป.ยอดเงินดรอป10104   \n",
       "...          ...  ..  ...       ...     ...                            ...   \n",
       "3896  12.03.2025  40  ...                          CDRP1 เงินสด Drop ลงเซฟ   \n",
       "3897  12.03.2025  40  ...                          CDRP1 เงินสด Drop ลงเซฟ   \n",
       "3898  12.03.2025  40  ...                          CDRP1 เงินสด Drop ลงเซฟ   \n",
       "3899  12.03.2025  40  ...                          CDRP1 เงินสด Drop ลงเซฟ   \n",
       "3900  12.03.2025  40  ...                          CDRP1 เงินสด Drop ลงเซฟ   \n",
       "\n",
       "0    AssignmentDate   DocDate POSNumber AmountCURR               FileName  \\\n",
       "2          20250301  20250103     10019       -991  1000020_25_03_13.XLSx   \n",
       "3          20250301  20250103     10032        -50  1000020_25_03_13.XLSx   \n",
       "4          20250301  20250103     10040     224167  1000020_25_03_13.XLSx   \n",
       "5          20250301  20250103     10085     136861  1000020_25_03_13.XLSx   \n",
       "6          20250301  20250103     10104      -2584  1000020_25_03_13.XLSx   \n",
       "...             ...       ...       ...        ...                    ...   \n",
       "3896       20250312  20251203     22220      65643  1000020_25_03_13.XLSx   \n",
       "3897       20250312  20251203     22413      34419  1000020_25_03_13.XLSx   \n",
       "3898       20250312  20251203     22430      71098  1000020_25_03_13.XLSx   \n",
       "3899       20250312  20251203     22442     160493  1000020_25_03_13.XLSx   \n",
       "3900       20250312  20251203     22466     151284  1000020_25_03_13.XLSx   \n",
       "\n",
       "0                      ETL_Date YYYYMM_DocDate  \n",
       "2    2025-03-31 20:02:23.208218         202503  \n",
       "3    2025-03-31 20:02:23.208218         202503  \n",
       "4    2025-03-31 20:02:23.208218         202503  \n",
       "5    2025-03-31 20:02:23.208218         202503  \n",
       "6    2025-03-31 20:02:23.208218         202503  \n",
       "...                         ...            ...  \n",
       "3896 2025-03-31 20:02:23.208218         202503  \n",
       "3897 2025-03-31 20:02:23.208218         202503  \n",
       "3898 2025-03-31 20:02:23.208218         202503  \n",
       "3899 2025-03-31 20:02:23.208218         202503  \n",
       "3900 2025-03-31 20:02:23.208218         202503  \n",
       "\n",
       "[3899 rows x 22 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"1000020_25_03_13.XLSx\"\n",
    "df = pd.read_excel(filename,skiprows=4).fillna(\"\").astype(\"str\").sum(axis=1).str.split(\"\\t\",expand=True)\n",
    "df_col = df.iloc[0].str.replace('.','').str.replace('G/L','G_L')\n",
    "df = df.iloc[2:]\n",
    "df.columns = df_col.str.strip().apply(lambda x: '-' if len(x) == 0 else x)\n",
    "df = df.iloc[:-2]\n",
    "df = df.drop(columns='-')\n",
    "df = df[df['G_L'].str.len()>=1]\n",
    "df['AssignmentDate'] = df['Assignment'].str[-8:]\n",
    "df['DocDate'] = pd.to_datetime(df['Doc Date'].str.replace(\".\",\"\"),format=\"%m%d%Y\").dt.strftime('%Y%m%d')\n",
    "df['POSNumber'] = df['Assignment'].str[:5]\n",
    "df['Amount in doc curr'] = df['Amount in doc curr'].str.strip()\n",
    "df['DocumentNo'] = df['DocumentNo'].str.strip()\n",
    "df['AmountCURR'] = df['Amount in doc curr'].str.extract(r'([^\\.]+)\\..*',expand=False).str.replace(r'[\\.,]',\"\",regex=True)\n",
    "df['FileName'] = filename\n",
    "df['ETL_Date'] = datetime.now()\n",
    "df['YYYYMM_DocDate'] = pd.to_datetime(df['Doc Date'],format='%d.%m.%Y').dt.strftime('%Y%m')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Date</th>\n",
       "      <th>DocDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>20250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>20250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>20250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>20250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>01.03.2025</td>\n",
       "      <td>20250103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>20251203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3897</th>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>20251203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>20251203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>20251203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>12.03.2025</td>\n",
       "      <td>20251203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0       Doc Date   DocDate\n",
       "2     01.03.2025  20250103\n",
       "3     01.03.2025  20250103\n",
       "4     01.03.2025  20250103\n",
       "5     01.03.2025  20250103\n",
       "6     01.03.2025  20250103\n",
       "...          ...       ...\n",
       "3896  12.03.2025  20251203\n",
       "3897  12.03.2025  20251203\n",
       "3898  12.03.2025  20251203\n",
       "3899  12.03.2025  20251203\n",
       "3900  12.03.2025  20251203\n",
       "\n",
       "[3899 rows x 2 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Doc Date', 'DocDate']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
