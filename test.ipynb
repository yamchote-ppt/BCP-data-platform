{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Union, List, Tuple, Any\n",
    "# from tabulate import tabulate\n",
    "\n",
    "class AuditLog:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = {column: None for column in self.columns}\n",
    "        self.log['STARTTIME'] = datetime.now() + timedelta(hours=7)\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.log)\n",
    "    \n",
    "class AuditLog_SPC(AuditLog):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId, TriggerType, TableName, functionName, ):\n",
    "        super().initialDetail({'PIPELINENAME': pipelineName, 'PIPELINERUNID': pipelineId, 'TRIGGERTYPE': TriggerType, 'TABLE_NAME': TableName, 'FUNCTION_NAME': functionName})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = AuditLog_SPC('SPC_UAT', 'factTest', 'auditTable', 'SilverLH', 'AuditLH')\n",
    "ad.initialDetail('testPL', '123', 'manual', 'factTest', 'testFunction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STATUS_ACTIVITY': 'Not start', 'TRIGGERTYPE': None, 'ERRORCODE': None, 'STARTTIME': datetime.datetime(2025, 1, 18, 4, 50, 58, 114075), 'ENDTIME': None, 'PIPELINERUNID': None, 'TABLE_NAME': None, 'ERRORMESSAGE': None, 'PIPELINENAME': None, 'AUDITKEY': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'FUNCTION_NAME': None}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenKeysDict:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = FrozenKeysDict(**{column: None for column in ['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log['PIPELINENAME'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIPELINENAME': 'xxx', 'PIPELINERUNID': None, 'TRIGGERTYPE': None, 'TABLE_NAME': None, 'FUNCTION_NAME': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'ERRORCODE': None, 'ERRORMESSAGE': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Cannot add new key: new'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myyy\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mFrozenKeysDict.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data:\n\u001b[1;32m---> 10\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot add new key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[key] \u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Cannot add new key: new'"
     ]
    }
   ],
   "source": [
    "log['new']='yyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logger:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "\n",
    "class AuditLog:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str, ...]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = logger(**{column: None for column in self.columns})\n",
    "        self.log['STARTTIME'] = datetime.now() + timedelta(hours=7)\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "\n",
    "    def setKey(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        self.setKey(initConfig)\n",
    "\n",
    "    def getKey(self):\n",
    "        return self.columns\n",
    "    \n",
    "    def getLog(self):\n",
    "        return self.log\n",
    "        \n",
    "    def __str__(self):\n",
    "        out = ''\n",
    "        for key in self.columns:\n",
    "            out += f'{key}: {self.log[key]}\\n'\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.log)\n",
    "\n",
    "\n",
    "class AuditLog_SPC(AuditLog):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId, TriggerType, TableName, functionName, ):\n",
    "        super().initialDetail({\n",
    "            'PIPELINENAME': pipelineName, \n",
    "            'PIPELINERUNID': pipelineId, \n",
    "            'TRIGGERTYPE': TriggerType, \n",
    "            'TABLE_NAME': TableName, \n",
    "            'FUNCTION_NAME': functionName\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'STATUS_ACTIVITY': 'Not start', 'TRIGGERTYPE': 'manual', 'ERRORCODE': None, 'STARTTIME': datetime.datetime(2025, 1, 18, 20, 19, 37, 815088), 'ENDTIME': None, 'PIPELINERUNID': '123', 'TABLE_NAME': 'factTest', 'ERRORMESSAGE': None, 'PIPELINENAME': 'testPL', 'AUDITKEY': None, 'COUNTROWSBEFORE': None, 'COUNTROWSAFTER': None, 'FUNCTION_NAME': 'testFunction'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 20:19:37.815088\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.functions import col, when, concat, lit, format_string,sum, upper, substring, expr, current_date, current_timestamp,to_timestamp,concat_ws, isnull, date_format, asc, trim, trunc, date_sub, year,coalesce, count, countDistinct, min, max\n",
    "from pyspark.sql.types import IntegerType, DecimalType, StringType, LongType, TimestampType, StructType, StructField, DoubleType, FloatType\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Union, List, Tuple, Any\n",
    "# from tabulate import tabulate\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"utils\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "class logger:\n",
    "    def __init__(self, **kwargs):\n",
    "        self._data = kwargs\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._data[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key not in self._data:\n",
    "            raise KeyError(f\"Cannot add new key: {key}\")\n",
    "        self._data[key] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        raise KeyError(f\"Cannot delete key: {key}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self._data)\n",
    "    \n",
    "# class Audit:\n",
    "#     def __init__(self):\n",
    "#         raise NotImplementedError('This is an abstract class')\n",
    "    \n",
    "#     def setKeys(self, initConfig: dict[str, Any]):\n",
    "#         assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "#         for column in initConfig:\n",
    "#             self.log[column] = initConfig[column]\n",
    "\n",
    "        \n",
    "\n",
    "class AuditLog_Fusion:\n",
    "    \n",
    "    def __init__(self, columns: Union[List[str], Tuple[str, ...]], WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        self.WS_ID = WS_ID\n",
    "        self.TABLE_NAME_to_check = TABLE_NAME_to_check\n",
    "        self.AUDIT_TABLE_NAME = AUDIT_TABLE_NAME\n",
    "        self.LH_ID_to_check = LH_ID_to_check\n",
    "        self.LH_ID_audit = LH_ID_audit if LH_ID_audit else LH_ID_to_check\n",
    "        self.schema = schema\n",
    "        self.fixColumns = {'STARTTIME','ENDTIME','AUDITKEY','STATUS_ACTIVITY'}\n",
    "        self.columns = tuple(set(columns).union(self.fixColumns))\n",
    "        \n",
    "        if self.schema:    \n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.schema}/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.schema}/{self.TABLE_NAME_to_check}'\n",
    "        else:\n",
    "            self.PATH_TO_AUDIT_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_audit}/Tables/{self.AUDIT_TABLE_NAME}'\n",
    "            self.PATH_TO_CHECKED_TABLE = f'abfss://{self.WS_ID}@onelake.dfs.fabric.microsoft.com/{self.LH_ID_to_check}/Tables/{self.TABLE_NAME_to_check}'\n",
    "    \n",
    "        self.log = logger(**{column: None for column in self.columns})\n",
    "        self.log['STARTTIME'] = datetime.now()\n",
    "        self.log['STATUS_ACTIVITY'] = 'Not start'\n",
    "\n",
    "    def setKeys(self, initConfig: dict[str, Any]):\n",
    "        assert set(initConfig.keys()).issubset(set(self.columns).difference()), f'initConfig must have the columns in {self.columns}'\n",
    "        for column in initConfig:\n",
    "            self.log[column] = initConfig[column]\n",
    "\n",
    "    def setKey(self, key: str, value: Any):\n",
    "        assert key in self.columns, f'key must be in {self.columns}'\n",
    "        self.log[key] = value\n",
    "        \n",
    "    def initialDetail(self, initConfig: dict[str, Any]):\n",
    "        self.setKeys(initConfig)\n",
    "\n",
    "    def getKey(self):\n",
    "        return self.columns\n",
    "    \n",
    "    def getLog(self):\n",
    "        return self.log\n",
    "        \n",
    "    def __str__(self):\n",
    "        out = ''\n",
    "        for key in self.columns:\n",
    "            out += f'{key}: {self.log[key]}\\n'\n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.log)\n",
    "    \n",
    "    def endSuccess(self):\n",
    "        self.log['STATUS_ACTIVITY'] = 'Fail'\n",
    "        self._endAuditLog()\n",
    "        print(self)\n",
    "        \n",
    "    def endFail(self, errorCode: str, errorMessage: str):\n",
    "        self.log['STATUS_ACTIVITY'] = 'Fail'\n",
    "        self.log['ERRORCODE'] = errorCode\n",
    "        self.log['ERRORMESSAGE'] = errorMessage\n",
    "        self._endAuditLog()\n",
    "        print(self)\n",
    "\n",
    "    def _endAuditLog(self):\n",
    "        # write to audit table\n",
    "        self.log['ENDTIME'] = datetime.now()\n",
    "        df = spark.createDataFrame([self.log])\n",
    "        df.write.mode('append').save(self.PATH_TO_AUDIT_TABLE)\n",
    "\n",
    "    def getAuditLogTable(self):\n",
    "        return spark.read.load(self.PATH_TO_AUDIT_TABLE)\n",
    "    \n",
    "    def countBefore(self, df):\n",
    "        self.log['COUNTROWSBEFORE'] = df.count()\n",
    "\n",
    "    def countAfter(self, df):\n",
    "        self.log['COUNTROWSAFTER'] = df.count()\n",
    "\n",
    "    def getAllPath(self):\n",
    "        return {'PATH_TO_AUDIT_TABLE':self.PATH_TO_AUDIT_TABLE, 'PATH_TO_CHECKED_TABLE':self.PATH_TO_CHECKED_TABLE}\n",
    "\n",
    "\n",
    "class AuditLog_SPC(AuditLog_Fusion):\n",
    "    \n",
    "    def __init__(self, WS_ID: str, TABLE_NAME_to_check:str, AUDIT_TABLE_NAME:str, LH_ID_to_check: str, LH_ID_audit: str = None, schema: str = None):\n",
    "        '''\n",
    "        - if `LH_ID_audit` is not given, it is  LH_ID_to_check automatically, i.e. audit table is in the same lakehouse as that of\n",
    "        - if using lakehouse with Schema, please provide `schema` parameter\n",
    "        '''\n",
    "        super().__init__(['PIPELINENAME', 'PIPELINERUNID', 'TRIGGERTYPE', 'TABLE_NAME', 'FUNCTION_NAME','COUNTROWSBEFORE', 'COUNTROWSAFTER', 'ERRORCODE', 'ERRORMESSAGE'] ,WS_ID, TABLE_NAME_to_check, AUDIT_TABLE_NAME, LH_ID_to_check, LH_ID_audit, schema)\n",
    "\n",
    "    def initialDetail(self,  pipelineName: str, pipelineId: str, TriggerType: str, TableName: str, functionName: str):\n",
    "        super().initialDetail({\n",
    "            'PIPELINENAME': pipelineName, \n",
    "            'PIPELINERUNID': pipelineId, \n",
    "            'TRIGGERTYPE': TriggerType, \n",
    "            'TABLE_NAME': TableName, \n",
    "            'FUNCTION_NAME': functionName\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 13:40:43.660532\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ad = AuditLog_SPC(WS_ID = 'SPC_UAT', TABLE_NAME_to_check = 'factTest', AUDIT_TABLE_NAME='auditTable', LH_ID_to_check='SilverLH', LH_ID_audit='AuditLH')\n",
    "ad.initialDetail(pipelineName = 'testPL', pipelineId = '123', TriggerType = 'manual', TableName = 'factTest', functionName = 'testFunction')\n",
    "print(ad)\n",
    "\n",
    "raise FileExistsError('Create you audit table first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: bigint]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoDf = spark.createDataFrame([{'a':1, 'b':2}, {'a':3, 'b':4}])\n",
    "demoDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS_ACTIVITY: Not start\n",
      "TRIGGERTYPE: manual\n",
      "ERRORCODE: None\n",
      "STARTTIME: 2025-01-18 13:39:16.665795\n",
      "ENDTIME: None\n",
      "PIPELINERUNID: 123\n",
      "TABLE_NAME: factTest\n",
      "ERRORMESSAGE: None\n",
      "PIPELINENAME: testPL\n",
      "AUDITKEY: None\n",
      "COUNTROWSBEFORE: None\n",
      "COUNTROWSAFTER: None\n",
      "FUNCTION_NAME: testFunction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notebookutils.fs.exists(ad.PATH_TO_AUDIT_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2940658051.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[71], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ad.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
